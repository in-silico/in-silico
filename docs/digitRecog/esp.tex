
\documentclass[10pt,journal]{IEEEtran}

\usepackage{mathtools}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subfig}
%\usepackage{picture}

\begin{document}
\title {Digit Recognition using Flusser and Hu moment invariants}
%\author {Sebastian Gomez}
\date {July of 2010}

	\markboth{VI Congreso Colombiano de Computacion,~Vol.~6, No.~1, December 2010}
	{Sebastian Gomez: Shape recognition using machine learning}

	\maketitle

	\begin{abstract}
		En el reconocimiento de caracteres o digitos usualmente se computan caracteristicas para entrenar un algoritmo de clasificación. Es deseable que esas caracteristicas sean invariantes a la rotación y al escalamiento para poder reconocer distintos tipos de letras e imagenes con distintos angulos de rotación. Por esta razón, para las aplicaciones de reconomiento optico de caracteres, la rotación y el escalamiento se consideran ruido.
		En este articulo se mostrara como clasificar digitos usando momentos invariantes propuestos por Hu y Flusser, utilizando como clasificador un metodo estadistico que utiliza la distribución normal multivariable y el teorema de bayes para computar las probabilidades posteriores.
	\end{abstract}
	
	\begin{IEEEkeywords}
	Artificial Intelligence, Machine learning, Character Recognition, Pattern Recognition,
	Probability Distribution, Parametric Statistics, Covariance Matrix
	\end{IEEEkeywords}
	
	
 
	Where $m(x,y)$ is the mean and $s(x,y)$ is the standard deviation of some area around $(x,y)$, and R is
	the maximum value of the standard deviation (128 for a gray scale document). If $f(x,y)$ is the original
	image of size $N*M$, the Breuel improvement using integral images consist on computing 
	the integral images of $f(x,y)$ and $f(x,y)^2$ in time  and use those integral images to
	compute $m(x,y)$ and $s(x,y)$ in constant time.
	
	\subsection{Segmentation of the image}
	Before being able to recognize characters, it is required to know where are the characters in the
	image. Lets assume that the characters are darker that its background, and that $I_2(x,y)$ is the
	binarized image as in Eq.\eqref{binEq}. Then it can be considered that a pixel $(x,y)$ is background
	if $I_2(x,y)=255$, so we will focus on those pixels where $I_2(x,y)=0$.\newline
	Lets say that a pixel $(x',y')$ is a neighbor pixel of $(x,y)$ if $|x-x'|=1$ or $|y-y'|=1$. Then we
	can define a connected component as a set of pixels $C=\{(x_i,y_i)\}$ where $\forall{(x_i,y_i)}, I_2(x_i,y_i)=0$
	and there exist a path from $(x_i,y_i)$ to $(x_j,y_j)$ for all $i,j$ moving only between neighbor
	pixels on $C$. The idea of connected components were proposed by O'Gorman \cite{docstrum93} in the
	docstrum algorithm for document analysis and page layout detection.
	
	\newtheorem{pixHasOneC}{Lemma}
	\begin{pixHasOneC}
		Each pixel $(x,y)$ such than $I_2(x,y)=0$ belongs to one and only one connected component $C$.
	\end{pixHasOneC}
	\begin{proof}
		To prove this lemma, lets suppose that the pixel $(x,y)$ belongs to more than one connected
		component. Lets then say that $C_i$ and $C_j$ are two different connected components that
		contains $(x,y)$, then there is a path from all $P_i \in C_i$ to $(x,y)$ and from all
		$P_j \in C_j$ to $(x,y)$.
		Then there is at least one path from all $P_i$ to all $P_j$, that path is 
		$P_i \rightarrow (x,y) \rightarrow P_j$. That means that $C_i$ and $C_j$ can be merged
		into one connected component.
	\end{proof}
	
	Then a flood-fill algorithm can be used to find a connected component given a pixel $(x,y)$ with $I_2(x,y)=0$. And
	with that we can get all the connected components of a page.
	
	
	\subsection{Feature extraction}
	Once the original image has been segmented into its connected components, an enclosing window can be
	computed for each connected component $C$. To get this window the values $max(x),min(x),max(y),min(y)$
	are computed for all $(x,y) \in C$.
	If the size of the window is smaller than some value or bigger that some other value, then that
	connected component is considered noise. For all the other connected components that have the
	valid size, the moment invariants of Hu and Flusser are computed.
	
	\section{Classification algorithm}
	Lets define some notation:
	\begin{itemize}
		\item $P(A)$ is the probability of A.
		\item $p(x)$ is the density of probability for variable x.
		\item $P(A|B)$ is the probability of A given B.
		\item $p(x|B)$ is the density of probability for variable x given B.
	\end{itemize}
	Say you have as input a d-dimensional vector $\vec{x}$, and it is normal distributed, then we have:
	\begin{equation}\label{multDens}
		p(x) = \frac{1}{(2\pi)^{d/2}|\Sigma|^\frac{1}{2}} e^{-\frac{1}{2}(\vec{x}-\vec{\mu})\Sigma^{-1}(\vec{x}-\vec{\mu})}
	\end{equation}
	This is notated as $\vec{x} \sim N(\vec{\mu},\Sigma)$ and means that $\vec{x}$ follows a multivariate 
	normal distribution with mean $\vec{\mu}$ and covariance matrix $\Sigma$. If for each class $C_i$ the mean $\vec{\mu_i}$
	and the covariance $\Sigma_i$ are computed, the density $p(x|C_i)$ is:
	\begin{equation}\label{multivariate}
		p(x|C_i) = \frac{1}{(2\pi)^{d/2}|\Sigma_i|^\frac{1}{2}} e^{-\frac{1}{2}(\vec{x}-\vec{\mu_i})\Sigma_i^{-1}(\vec{x}-\vec{\mu_i})}
	\end{equation}
	
	The multivariate normal distribution can be used as a classifier algorithm. The objective is to find $P(C_i|x)$,
	that is the posterior probability of the input vector $\vec{x}$ to belong to the class $C_i$. If we have $P(C_i|x)$
	for each class $i$, the class of the given instance can be chosen taking the class with maximum posterior
	probability. To compute the posterior probabilities has the advantage that some corrections can be performed in
	a later stage, for example, by using a dictionary. \newline
	As shown in Eq. \eqref{multivariate}, what we can compute is $p(x|C_i)$, that is the probability
	density of the random variable $\vec{x}$ given de class $C_i$; What we want is the probability that
	a given $\vec{x}$ belongs to some class $P(C_i|\vec{x})$ (That is the posterior probability). For
	that we use the Bayes theorem:
	\begin{equation}\label{Bayes}
		P(C_i|\vec{x}) = \frac{p(\vec{x}|C_i)P(C_i)}{p(x)} = \frac{p(\vec{x}|C_i)P(C_i)}{ \sum_j{p(\vec{x}|C_j)P(C_j)} }
	\end{equation}
	
	From Eq.\eqref{Bayes} it can be seen immediately that $\sum_i{P(C_i|\vec{x})}=1$, and that $0 \le P(C_i|\vec{x}) \le 1$
	for all classes.
	
	\subsection{Training}
	We have already seen how the classification can be performed once the parameters $\vec{\mu_i}$ and $\Sigma_i$ are
	computed for each class $C_i$. So the training consist on computing the parameters $\vec{\mu_i}$ and $\Sigma_i$ from
	a training set $T$. First, the training set must be splited in different sets $T_i$, where each $T_i$ contains only
	instances whose class is $C_i$. Then, the parameters are computed as:
	\begin{equation}\label{multiParams}
	\begin{aligned}
		\vec{\mu_i} &= E[ \vec{x_i} ] \\
		\Sigma_i(a,b) &= E[ (\vec{x_i}-\vec{\mu_a})(\vec{x_i}-\vec{\mu_b}) ]
	\end{aligned}
	\end{equation}
	Where $\vec{x_i}$ are all the instances of the subset $T_i$, $E[x]$ is the expected value or mean of x and
	$\Sigma_i(a,b)$ is the value at row $a$ and column $b$ of matrix $\Sigma_i$.
	
	\subsection{Multivariate method simplifications}
	The number of parameters for a multivariate normal distribution grows quadratically with the input 
	dimensions d, since the covariance matrix is of size $d \times d$. This becomes a problem when 
	there is not enough data to estimate the parameters properly. 
	Lets say that from a sample	of three instances we want compute the mean and the variance 
	of the population; It is clear that	the values can be computed, but probably it doesn't 
	represent well the parameters of the population. The same way, when more parameter are to
	be computed more data is needed for good generalization.\newline
	Then, sometimes it may be desirable to reduce the amount of parameters to achieve better
	generalization. In the case of a multivariate method, one approach to simplify the model
	is to use a shared covariance matrix for all classes. The posterior probabilities can be
	computed the same way, and to choose the class with the highest posterior probability. That
	is equivalent to compute the Mahalanobis distance between the input and each class and
	to choose the one with smaller distance. Let $d_i$ be the Mahalanobis distance of 
	the input $\vec{x}$ to the class $C_i$, then:
	\[ d_i = (\vec{x}-\vec{\mu_i})\Sigma(\vec{x}-\vec{\mu_i}) \]
	Note that when the covariance matrix is equal to the identity matrix, then the Mahalanobis
	distance becomes the euclidean distance squared.\newline
	A mixture between a full multivariate model and a multivariate model with shared matrix can
	yield good results when there is risk to have a determinant of the covariance matrix very
	close to zero. Lets call this model as multivariate normal distribution with semi-shared
	covariance matrix, and lets define it as:
	\begin{equation}\label{semishared}
		p(x|C_i) = \frac{1}{(2\pi)^{d/2}|\Sigma|^\frac{1}{2}} e^{-\frac{1}{2}(\vec{x}-\vec{\mu_i})\Sigma_i^{-1}(\vec{x}-\vec{\mu_i})}
	\end{equation}
	Where all variables have the same meaning as in Eq.\eqref{multivariate}. Note that the only
	difference between Eq.\eqref{multivariate} and Eq.\eqref{semishared} is that the covariance
	matrix in the coefficient whose determinant is computed is shared $\Sigma$; But the
	covariance matrix in the exponent $\Sigma_i$ remains the same (Is not shared).
	
	\section{Experiments and results}
	One page per class (digit) with 491 digit instances was printed. Five pictures were taken to
	each of those pages with different rotation angles between -30$^{\circ}$ and 30$^{\circ}$ 
	approximately. Each picture were binarized with the Sauvola-Breuel method and segmented with
	the connected component above explained. Each component where randomly chosen to belong to 
	the training or validation set with probabilities 70\% and 30\% respectively.\newline
	A multivariate normal distribution was trained with the
	Flusser and Hu moments of the components on the training set. Then the accuracy was tested
	with the components in the validation set. The reason to split all data in the training and
	validation set, is that the objective of the machine learning is to get a model with the 
	ability to generalize over instances that were not shown in the training process.\newline
	The results of the accuracy over the instances in the validation set are shown in table 
	\ref{exp1}. It can be seen that in general, the multivariate normal distribution with
	semi-shared covariance matrix yields better result for this problem and those moment invariants
	than the full multivariate normal distribution (The one with different covariance matrix 
	and coefficient	for each class). The most probable reason for this is that the digits with
	some degree of symmetry have some invariants with value zero, and then the determinant of
	the covariance matrix become also very small for those digits.\newline
	
	\begin{table}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		* & \multicolumn{2}{|c|}{Semi-shared covariance matrix} &
			\multicolumn{2}{|c|}{Independent covariance matrix} \\
		\hline
		Character & Hu & Flusser & Hu & Flusser \\
		\hline
		0 & 99.34\% & 99.21\% & 99.86\% & 100.00\% \\
		1 &	100.00\% & 100.00\% & 100.00\% & 100.00\% \\
		2 &	100.00\% & 100.00\% & 91.21\% & 93.91\% \\
		3 &	100.00\% & 100.00\% & 100.00\% & 100.00\% \\
		4 &	100.00\% & 100.00\%	& 100.00\% & 96.41\% \\		
		5 & 97.97\% & 99.32\% & 26.95\% & 38.95\% \\ 
		6 & 91.65\% & 86.96\% & 93.61\% & 86.83\% \\
		7 & 100.00\% & 100.00\% & 100.00\% & 99.87\% \\
		8 & 85.53\% & 92.97\% & 96.76\% & 99.44\% \\
		9 &	89.52\% & 81.06\% & 65.40\% & 65.03\% \\
		\hline
		Total & 96.38\% & 95.83\% & 87.21\% & 87.84\% \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Accuracy of the classifier algorithm in the first experiment}	
	\label{exp1}
	\end{table}
	
	\subsection{Problem of rotation invariance}
	Note that for the semi-shared matrix case, the digits 6 and 9 had the lowest results
	(Table \ref{exp1}), in fact, they were confusing among themselves. Remember that the
	features are invariant to rotation; A situation when humans do digit recognition with
	rotation invariant features, could be when playing pool, and the digits marked in the
	balls should be recognized independent of their rotation. In that case even humans have
	problems to recognize between the balls 6 and 9, and need an underscore to perform the
	recognition properly. This problem happens because of the mutual symmetry of those
	digits, and in the case of digits or characters in a printed document, there isn't usually
	a reference like the underscore; So lets constrain the document to have a rotation angle
	between $-45^\circ$ and $45^\circ$, to its right position. If the document has a bigger
	rotation angle, it can always be corrected by rotating the image multiples of $90^\circ$
	without any loose of data, if a proper reference is given to know the right rotation. \newline
	With those constrains, we propose the following method to solve the mutual symmetry problem.
	Lets say that there is a mutual symmetry problem between classes $C_i$ and $C_j$, and we
	want to classify an image $f(x,y)$ in one of those classes. Then we know that a rotation of
	180$^\circ$ of an instance of class $C_i$ is similar to an instance of class $C_j$. Lets
	also define $f_i(x,y)$ as an instance of class $C_i$ that represents well the instances
	of class $C_i$. So we would choose $f(x,y)$ to belong to $C_i$ if the rotation angle with
	respect to $f_i(x,y)$ is less than 90$^\circ$, if greater that 90$^\circ$ we would choose
	$C_j$, and if it is exactly equal to 90$^\circ$ no assumption can be made. \newline
	Let $C'_{pq}$ be the average of the complex moments of order $p+q$ of the instances of
	class $C_i$, and $C_{pq}$ be the complex moment of order $p+q$ of the image $f(x,y)$.
	Using Eq.\eqref{rotProp} we would have:
	\begin{equation}\label{angleDetection1}
		\frac{C'_{pq}}{C_{pq}} = e^{-i(p-q)\alpha}
	\end{equation}
	Eq.\eqref{angleDetection1} shows that it is always possible to compute the angle $\alpha$ 
	between	$f(x,y)$ and $f_i(x,y)$ by dividing its complex moments of order $p+q$ if 
	$p \neq q$ and the magnitude of the moments its different from zero. Lets define
	$n=\frac{C'_{pq}}{C_{pq}}$. The complex number $n$ should have unitary magnitude,
	in the theory that is guaranteed, but with real images that supposition doesn't hold.
	We normalized the complex number $n'=\frac{n}{|n|}$ before proceeding. Using the Euler
	identity in Eq.\eqref{angleDetection1}, we have:
	\begin{equation}\label{angleDetection2}
		n' = \cos((p-q)\alpha) - i\sin((p-q)\alpha)
	\end{equation}
	If we take $n'$ as a vector, it would move in a unitary circle around the origin
	depending on the value of $\alpha$. If we take $\alpha=0$ as a reference vector then we 
	would have the vector $\vec{R}=(1,0)$. Note that $\vec{R}$ is a normal vector to a classifier
	plane $A$.
	\begin{center}
	\setlength{\unitlength}{1cm}
	\begin{picture}(2,2)
	\put(1,1){\circle{2}}
	\put(1,1){\vector(1,0){0.9}}
	\put(2,1){$\vec{R}$}
	\put(1,0.1){\line(0,1){1.8}}
	\put(1,0){A}
	\put(1,1){\vector(1,2){0.4}}
	\put(1.5,1.8){$n'$}
	\end{picture}
	\end{center}
	Lets define the positive side of the plane $A$ as the side pointed by $\vec{R}$. Then
	we would choose $f(x,y)$ to belong to $C_i$ if $n'$ is at the positive side of the
	plane $A$, we choose $C_j$ if $n'$ is at the negative side of $A$ and there is no clue
	if $n'$ is on the plane $A$. We compute in which side of the plane is the vector $n'$ by
	computing its dot product with the normal vector $\vec{R}$. Note that:
	\[ \vec{R}.n' = real(n') \]
	Where $real(x)$ if a function that returns the real part of a complex number $x$. The
	classification is then performed as:
	\begin{equation}\label{angleClassif}
		Class = \left\{ \begin{array}{ll}
		C_i   & \mbox{if $real(n') > 0$} \\
		C_j & \mbox{otherwise}
	\end{array} \right. 
	\end{equation}
	The results after implementing this method for the digits 6 and 9 are shown in table
	\ref{exp2}.
	
	\begin{table}
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		* & \multicolumn{2}{|c|}{Semi-shared covariance matrix} \\
		\hline
		Character & Hu Accuracy & Flusser Accuracy \\
		\hline
		6 & 99.47\% & 99.48\%	\\
		9 &	99.49\% & 96.72\% \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Improved accuracy for digits 6 and 9}	
	\label{exp2}
	\end{table}Y alguna chica especial?
	
	\section{Results and conclusions}
	It can be seen that after the method for solving the mutual symmetry problem, the accuracy
	was improved for those specific numbers. It was also noted that the misclassified instances
	of both digits 6 and 9, were no longer confusing among themselves. To improve even more the
	accuracy of the OCR system, it was also tested with the flusser moment for objects with 2-fold
	rotation symmetry. The final results are show in the table \ref{exp3} for the semishared 
	covariance matrix classification and in table \ref{exp4} for the independent covariance matrix
	method. 
	
	\begin{table}
	\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		* & \multicolumn{3}{|c|}{Semi-shared covariance matrix} \\
		\hline
		Character & Hu & Flusser & 2-Fold flusser\\
		\hline
		0 & 99.34\% & 99.21\% & 100.00\% \\
		1 &	100.00\% & 100.00\% & 100.00\% \\
		2 &	100.00\% & 100.00\% & 100.00\% \\
		3 &	100.00\% & 100.00\% & 100.00\% \\
		4 &	100.00\% & 100.00\%	& 100.00\% \\		
		5 & 97.97\% & 99.32\% & 100.00\% \\ 
		6 & 99.47\% & 99.48\% & 100.00\% \\
		7 & 100.00\% & 100.00\% & 100.00\% \\
		8 & 85.53\% & 92.97\% & 100.00\% \\
		9 &	99.49\% & 96.72\% & 100.00\% \\
		\hline
		Total & 96.38\% & 95.83\% & 100.00\% \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Accuracy of the classifier algorithm in the first experiment}	
	\label{exp3}
	\end{table}
	
	\begin{table}
	\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		* & \multicolumn{3}{|c|}{Independent covariance matrix} \\
		\hline
		Character & Hu & Flusser & 2-Fold flusser\\
		\hline
		0 & 99.86\% & 100.00\% & ND\\
		1 &	100.00\% & 100.00\% & ND \\
		2 &	91.21\% & 93.91\% & ND \\
		3 &	100.00\% & 100.00\% & ND \\
		4 &	100.00\% & 96.41\% & ND \\		
		5 & 26.95\% & 38.95\% & ND \\ 
		6 & 93.61\% & 99.48\% & ND \\
		7 & 100.00\% & 99.87\% & ND \\
		8 & 96.76\% & 99.44\% & ND \\
		9 &	65.40\% & 96.72\% & ND \\
		\hline
		Total & 87.21\% & 87.84\% & 100.00\% \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Accuracy of the classifier algorithm in the first experiment}	
	\label{exp4}
	\end{table}
	
	The fact that the semi-shared produced always a better result for this application may
	be because of the unstability of the determinant of the covariance matrix of each class.
	It is important to say that the result was 100\% for the flusser moments with 2-fold 
	rotation symmetry doesn't mean that it would work perfectly under real circumstances.
	The pictures were taken with the same camera and resolution, of the same pages and
	with similar lighting conditions, an using only one font type. But as all the methods
	were tested under the same conditions, their results can be compared.
\end{document}
